{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marrytorichelli/DeepSchoolHW/blob/main/hw1_Sheshenina_Mariia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wy-Ue-RLEFzn"
      },
      "source": [
        "# Домашнее задание (50 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "COBmhBpJEFzp"
      },
      "source": [
        "В этом домашнем задании вы познакомитесь с основами NLP, научитесь обрабатывать тексты.\n",
        "\n",
        "В местах, где используется `...` (elipsis), требуется заменить его на код.\n",
        "\n",
        "Установим необходимые зависимости:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-27T19:21:30.570036Z",
          "start_time": "2024-11-27T19:21:27.452197Z"
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7qvvkCtEFzq",
        "outputId": "3a85a4b4-df88-470c-ca5d-1b5302ca2b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!pip install nltk tqdm seqeval scikit-learn datasets numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-27T19:21:40.162946Z",
          "start_time": "2024-11-27T19:21:40.157819Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U6srT4dEFzs",
        "outputId": "13d51c01-b371-400d-8a4a-8960d91bbc9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict, Tuple, Callable\n",
        "from collections import Counter, defaultdict\n",
        "import unicodedata\n",
        "import random\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")\n",
        "print(\"Done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-V6NDJrMEFzs"
      },
      "source": [
        "## Токенизация (15 баллов)\n",
        "\n",
        "Токенизация - это процесс преобразования текста в набор токенов.\n",
        "Наивная реализация разбивает текст по пробелам. Более умные реализации учитывают пунктуацию.\n",
        "\n",
        "### Библиотека NLTK (2 балла)\n",
        "\n",
        "Научимся работать с токенизацией NLTK, где уже реализована работа с пунктуацией.\n",
        "\n",
        "https://www.nltk.org/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnxWgXGZEFzt",
        "outputId": "2909532f-2e32-423b-e047-b9e3334b1bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_tokenize OK\n"
          ]
        }
      ],
      "source": [
        "def tokenize(text: str, language: str = \"english\") -> List[str]:\n",
        "    # используйте функцию nltk.word_tokenize для разбиения текста на токены\n",
        "    # https://www.nltk.org/api/nltk.tokenize.word_tokenize.html\n",
        "    # raise NotImplementedError()\n",
        "\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "assert tokenize(\"\") == []\n",
        "assert tokenize(\"Hello, world!\") == [\"Hello\", \",\", \"world\", \"!\"]\n",
        "assert tokenize(\"EU rejects German call to boycott British lamb.\") == [\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]\n",
        "print(\"test_tokenize OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lyqjAL9h_28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcmyOic5EFzt"
      },
      "source": [
        "### Нормализация (3 балла)\n",
        "\n",
        "Добавим нормализацию после токенизации. Пробуем [лемматизацию](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.wordnet.WordNetLemmatizer) , [стемминг](https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.EnglishStemmer) и [юникод](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize) нормализацию. Напишем функцию, которая будет принимать на вход токен после токенизации, нормализовать в NFC юникод форму, переводит в нижний регистр, лемматизирует слово и, если слово не изменилось после лемматизации, применяет стемминг.\n",
        "\n",
        "\n",
        "Создайте функцию `normalize`:\n",
        "   - Функция `normalize` должна принимать строку `token` и возвращать нормализованный токен.\n",
        "   - Примените к токену Unicode нормализацию с помощью `unicode_nfc_normalizer`.\n",
        "   - Преобразуйте токен в нижний регистр.\n",
        "   - Примените лемматизацию с помощью `lemmatizer`.\n",
        "   - Если лемматизированный токен отличается от исходного, верните его. В противном случае, примените стемминг с помощью `stemmer` и верните результат."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4f8db1vgxnSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxqvuMohEFzu",
        "outputId": "053e426f-1032-4a65-cda4-769e86653ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_normalize OK\n"
          ]
        }
      ],
      "source": [
        "stemmer = EnglishStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "unicode_nfc_normalizer = lambda token: unicodedata.normalize(\"NFC\", token)\n",
        "\n",
        "\n",
        "def normalize(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Нормализует токен, применяя Unicode нормализацию, преобразование в нижний регистр,\n",
        "    лемматизацию и стемминг при необходимости.\n",
        "\n",
        "    :param token: Токен для нормализации\n",
        "    :return: Нормализованный токен\n",
        "    \"\"\"\n",
        "    # raise NotImplementedError()\n",
        "    token_norm = unicode_nfc_normalizer(token)\n",
        "    token_norm = token_norm.lower()\n",
        "    lemma = lemmatizer.lemmatize(token_norm)\n",
        "    if lemma!= token_norm:\n",
        "      return lemma\n",
        "\n",
        "    return stemmer.stem(token_norm)\n",
        "\n",
        "\n",
        "test_tokens = [\"Worlds\", \"churches\", \"Helping\"]\n",
        "assert [normalize(token) for token in test_tokens] == [\"world\", \"church\", \"help\"]\n",
        "print(\"test_normalize OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdUMWY6wEFzu"
      },
      "source": [
        "### Добавляем Словарь (10 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaP0hqALEFzu"
      },
      "source": [
        "Современные токенайзеры не только разбивают строки на токены, но и преобразуют последовательность токенов в последовательность числел. Объединим функцию токенизации, нормализации и отображения из токенов в индексы в один объект токенайзера.\n",
        "\n",
        "Напишите класс `Tokenizer` для токенизации и нормализации текста.\n",
        "\n",
        "Построение словаря:\n",
        "   - Создайте метод `_build_vocabulary`, который принимает список текстов `texts` и обновляет словарь токенов.\n",
        "   - Для каждого текста:\n",
        "     - Токенизируйте и нормализуйте текст.\n",
        "     - Обновите счетчик вхождений слов.\n",
        "   - После обработки всех текстов для каждого слова, которое встречается не менее `min_count` раз, добавьте слово в словарь `word2idx` и список `idx2word`.\n",
        "\n",
        "Кодирование и декодирование:\n",
        "   - Создайте метод `encode_word`, который принимает слово `word` и возвращает его индекс с применением нормализации.\n",
        "   - Создайте метод `encode`, который принимает текст `text` и возвращает список индексов токенов.\n",
        "   - Создайте метод `decode`, который принимает список индексов `input_ids` и возвращает текст, вставляя пробелы между токенами.\n",
        "\n",
        "> Note: для функций, которые могут долго исполнятся (`_build_vocab`), рекомендуется использовать библиотеку tqdm."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDW26uRM7vvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A-j3s4vEFzv",
        "outputId": "e0471c80-600e-4423-c06e-d2deb22c8149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_tokenizer OK\n"
          ]
        }
      ],
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            texts: List[str],\n",
        "            min_count: int = 1,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Инициализация токенизатора.\n",
        "\n",
        "        :param texts: список текстов для построения словаря\n",
        "        :param tokenize_fn: функция для токенизации текста\n",
        "        :param normalize_fn: функция для нормализации токенов\n",
        "        :param min_count: минимальное количество вхождений слова для включения в словарь\n",
        "        \"\"\"\n",
        "        self.min_count = min_count\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.unk_token_id = 3\n",
        "        self.idx2word = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n",
        "        self.word2count = Counter()\n",
        "        self._build_vocabulary(texts)\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "\n",
        "      return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n",
        "\n",
        "    def _build_vocabulary(self, texts: List[str]):\n",
        "        \"\"\"\n",
        "        Построение словаря на основе списка текстов.\n",
        "\n",
        "        :param texts: список текстов\n",
        "        \"\"\"\n",
        "        # Проходимся по всем текстам, токенизируем текст,\n",
        "        # нормализуем токены и обновляем self.word2count\n",
        "        for text in texts:\n",
        "          tokens = self._tokenize(text)\n",
        "          for token in tokens:\n",
        "            self.word2count[token] += 1\n",
        "\n",
        "\n",
        "\n",
        "        # Теперь у нас есть заполненный self.word2count\n",
        "        # ключи - нормализованные токены, значения - их встерчаемость в тексте\n",
        "        # нужно добавить в словарь новый токен (обновить self.word2idx и self.idx2word),\n",
        "        # если его встречаемость не меньше self.min_count\n",
        "\n",
        "        for word, count in self.word2count.items():\n",
        "          if count >= self.min_count and word not in self.word2idx:\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "\n",
        "\n",
        "    def encode_word(self, text: str) -> int:\n",
        "        \"\"\"\n",
        "        Кодирование слова в индекс с применением нормализации.\n",
        "\n",
        "        :param text: слово\n",
        "        :return: индекс слова\n",
        "        \"\"\"\n",
        "        token = text.lower()\n",
        "        return self.word2idx.get(token, self.unk_token_id)\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"\n",
        "        Кодирование текста в набор индексов.\n",
        "\n",
        "        :param text: текст\n",
        "        :return: набор индексов токенов\n",
        "        \"\"\"\n",
        "        tokens = self._tokenize(text)\n",
        "        return [self.encode_word(token) for token in tokens]\n",
        "\n",
        "\n",
        "    def decode(self, input_ids: List[int]) -> str:\n",
        "        \"\"\"\n",
        "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
        "\n",
        "        :param input_ids: набор индексов токенов\n",
        "        :return: текст\n",
        "        \"\"\"\n",
        "        return ' '.join(self.idx2word[idx] if idx <len(self.idx2word) else \"<UNK>\" for idx in input_ids)\n",
        "        # tokens = [self.idx2word[idx] for idx in input_ids]\n",
        "        # return \" \".join(tokens)\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Возвращает количество уникальных токенов в словаре.\n",
        "\n",
        "        :return: количество уникальных токенов\n",
        "        \"\"\"\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def __contains__(self, item: str) -> bool:\n",
        "        \"\"\"\n",
        "        Проверяет, содержится ли слово в словаре.\n",
        "\n",
        "        :param item: слово\n",
        "        :return: True, если слово содержится в словаре, иначе False\n",
        "        \"\"\"\n",
        "        return item in self.word2idx\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Возвращает строковое представление словаря.\n",
        "\n",
        "        :return: строковое представление словаря\n",
        "        \"\"\"\n",
        "        return str(self.word2idx)\n",
        "\n",
        "\n",
        "corpus = [\"Hello, world!\", \"I love Python!\"]\n",
        "tokenizer = Tokenizer(corpus, min_count=1)\n",
        "encoded = tokenizer.encode(\"Hello, Python! I love you\")\n",
        "assert tokenizer.decode(encoded) == \"hello , python ! i love <UNK>\"\n",
        "print(\"test_tokenizer OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEilJ5zfEFzw"
      },
      "source": [
        "## TF-IDF (20 баллов)\n",
        "\n",
        "\n",
        "### Класс TFIDF (10 баллов)\n",
        "\n",
        "Создайте класс `TFIDF` для вычисления TF-IDF значений.\n",
        "\n",
        "Вам нужно реализовать следующие функции:\n",
        "### add_doc\n",
        "0. Увеличиваем счетчик num_docs - число документов в обучающей выборке\n",
        "1. Токенизируем текст\n",
        "2. Берем уникальные токены\n",
        "3. Обновляем self.term2num_docs - массив, в котором для каждого токена хранится число того, в скольких уникальных документах этот токен встречается. Токен `<UNK>` игнорируем.\n",
        "\n",
        "### idf\n",
        "Считаем логарифмированный inverse document frequency для документа\n",
        "\n",
        "$$\n",
        "idf = -log \\frac {n_t + 1} {N}\n",
        "$$\n",
        "где $n_t$ - в сколькиг документах встречается токен, $N$ - число различных документов. За эти параметры у нас отвечают параметры `self.term2num_docs` и `self.num_docs`. (единицы мы добавляем, чтобы избежа\n",
        "\n",
        "### predict\n",
        "1. Получаем набор документов, на выходе генерируем numpy матрицу tf-idf размера len(docs) x len(vocabulary)\n",
        "2. Токенизируем каждый текст\n",
        "3. Для каждого уникального токена в тексте (кроме `<UNK>`) считаем tf - как часто данный токен встречается во всем тексте (с учетом дублей и `<UNK>` токена!)\n",
        "4. Для каждого токена считаем idf из одноименной функции\n",
        "5. Заполняем соответствующие элементы массива\n",
        "6. Нормализуем вектор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZAugWUgsEFzw"
      },
      "outputs": [],
      "source": [
        "class TFIDF:\n",
        "  def __init__(self, tokenizer: Tokenizer) -> None:\n",
        "        \"\"\"\n",
        "        Инициализация TFIDF.\n",
        "\n",
        "        :param tokenizer: токенизатор для преобразования текста в токены\n",
        "        :param default_idf: значение IDF для неизвестных токенов\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_docs = 0\n",
        "        self.term2num_docs = [0 for _ in tokenizer.word2idx]  # для подсчёта IDF\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self) -> int:\n",
        "        \"\"\"\n",
        "        Возвращает размер словаря.\n",
        "\n",
        "        :return: размер словаря\n",
        "        \"\"\"\n",
        "        return len(self.tokenizer)\n",
        "\n",
        "  def add_doc(self, doc: str) -> None:\n",
        "      \"\"\"\n",
        "      Добавляет документ в модель TFIDF.\n",
        "      1. Увеличиваем счетчик числа документов\n",
        "      2. Токенизируем текст\n",
        "      3. Для всех уникальных токенов обновляем self.term2numdocs для подсчета IDF\n",
        "\n",
        "      :param doc: документ для добавления\n",
        "      \"\"\"\n",
        "      self.num_docs += 1\n",
        "      token_ids = [tid for tid in set(self.tokenizer.encode(doc)) if tid != self.tokenizer.unk_token_id]\n",
        "      for tid in token_ids:\n",
        "        self.term2num_docs[tid] += 1\n",
        "\n",
        "\n",
        "  def fit(self, docs: List[str]) -> None:\n",
        "      \"\"\"\n",
        "      Обучает модель TFIDF на корпусе docs.\n",
        "\n",
        "      :param docs: корпус для обучения\n",
        "      \"\"\"\n",
        "      for doc in docs:\n",
        "          self.add_doc(doc)\n",
        "\n",
        "  def predict(self, docs: List[str]) -> np.ndarray:\n",
        "      \"\"\"\n",
        "      Предсказывает TFIDF значения для списка документов.\n",
        "      1. Создаем np.array размерности (len(docs), vocab_size)\n",
        "      2. Для каждого документа\n",
        "          а. Токенизируем его\n",
        "          б. Считаем tf для каждого токена кроме unk\n",
        "          в. Считаем idf для каждого токена кроме unk\n",
        "          г. Заполняем соответствующее значение в матрице\n",
        "      3. Нормализуем матрицу по размерности словаря (по строкам). При нормализации,\n",
        "      чтобы избежать деления на 0 делите не на норму ветктора, а на норму вектора + 1e-5\n",
        "\n",
        "      :param docs: список документов\n",
        "      :return: матрица TFIDF значений\n",
        "      \"\"\"\n",
        "      # создаем numpy массив нулей размера len(docs) на размерность словаря\n",
        "      result = np.zeros((len(docs), self.vocab_size))\n",
        "      # для каждого документа будем считать его вектор tf-idf\n",
        "      for doc_idx, document_text in enumerate(docs):\n",
        "        token_ids = [tid for tid in self.tokenizer.encode(document_text) if tid != self.tokenizer.unk_token_id]\n",
        "        if not token_ids:\n",
        "          continue\n",
        "        # считаем tf для каждого токена\n",
        "        tf = Counter(token_ids)\n",
        "        for tid, freq in tf.items():\n",
        "          result[doc_idx, tid] = freq * self.idf(tid)\n",
        "        # считаем idf для каждого токена\n",
        "        idf = np.array([self.idf(tid) for tid in token_ids])\n",
        "      norm = np.linalg.norm(result, axis=1, keepdims=True)\n",
        "      # нормализуем документ\n",
        "      result = result / (norm + 1e-5)\n",
        "      return result\n",
        "\n",
        "  def idf(self, token_id: int) -> float:\n",
        "      \"\"\"\n",
        "      Вычисляет IDF (обратную частоту документа) для термина.\n",
        "\n",
        "      :param term: термин\n",
        "      :return: IDF значение\n",
        "      \"\"\"\n",
        "      return -np.log((self.term2num_docs[token_id] + 1) / self.num_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpBeJpaxEFzw",
        "outputId": "3249875f-324f-4fb5-8b0b-2fae1e6405e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_tfidf OK\n"
          ]
        }
      ],
      "source": [
        "corpus = [\n",
        "    \"hello there\", \"my favourite frut is\", \"i love bananas\", \"hello mama\", \"I need to eat\",\n",
        "    \"how can I get to the studio\", \"bottom gear\"\n",
        "]\n",
        "tokenizer = Tokenizer(corpus, min_count=1)\n",
        "tfidf = TFIDF(tokenizer)\n",
        "tfidf.fit(corpus)\n",
        "\n",
        "assert not np.any(tfidf.predict([\"all tokens abscent from vocab should be zeros vector\"]))\n",
        "reference = np.zeros((1, len(tfidf.tokenizer)))\n",
        "reference[0, tfidf.tokenizer.encode_word(\"hello\")] = 2 / 3 * -np.log(3/7)\n",
        "reference[0, tfidf.tokenizer.encode_word(\"mama\")] = 1 / 3 * -np.log(2/7)\n",
        "reference /= 0.7024715222440031\n",
        "assert np.allclose(tfidf.predict([\"hello hello mama\"]), reference)\n",
        "print(\"test_tfidf OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9L1mDJoEFzw"
      },
      "source": [
        "## Классификация с помощью TF-IDF - 10 баллов\n",
        "В этом задании предлагается обучить с помощью полученного векторизатора TF-IDF логистическую регресиию.\n",
        "Для этого мы возьмем корпус IMDB - отзывы фильмов. Это задача бинарной классификации, в которой нужно определить - положительный отзыв или отрицательный.\n",
        "\n",
        "Задача будет решаться следующими этапами:\n",
        "1. Загружаем текстовый корпус, обучаем словарь и TFIDF\n",
        "2. Векторизуем корпус текстов\n",
        "3. По векторизованному корпусу и меткам текстов обучаем логистическую регрессию, смотрим на качество на тесте"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade datasets fsspec pyarrow pandas\n"
      ],
      "metadata": {
        "id": "XprtPGC6oPkO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZqbQ2btEFzx",
        "outputId": "80cdfe45-71ba-412b-da7a-e4012a063819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "# загружаем датасет\n",
        "from datasets import load_dataset\n",
        "\n",
        "imdb = load_dataset(\"imdb\")\n",
        "print(imdb[\"train\"][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = [imdb[\"train\"][i] for i in range(10_000, 15_000)]\n",
        "test_dataset = [imdb[\"test\"][i] for i in range(10_000, 15_000)]\n",
        "\n",
        "print(\"Label\")\n",
        "print(train_dataset[0][\"label\"])\n",
        "print(\"Text\")\n",
        "print(train_dataset[0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG4gRhx5n0yr",
        "outputId": "afc5bfca-d331-4c43-9cf4-ff391b376391"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "0\n",
            "Text\n",
            "Someone actually gave this movie 2 stars. There's a very high chance they need immediate professional help as anyone who doesn't spend 30 seconds to see if you can award no stars is quite literally scary.<br /><br />This film is ... well ... I guess it's pretty much some kind of attempt at a horrible porn / snuff movie with no porn or no real horrible bits (apart from the acting, plot, story, sets, dialogue and sound). I wrongly assumed it was about zombies. <br /><br />Watching it is actually quite scary in fairness; you're terrified someone will come over and you'll never be able to describe what it is and they'll go away thinking you're a freak that watches home-made amateur torture videos or something along those lines. <br /><br />I'm so taken aback I'm writing this review on my mobile so I don't forget to attempt to bring the rating down further than the current 1.6 to save others from the same horrible fate that I just suffered. <br /><br />I worst film I've ever seen and I can say (with hand on heart) it will never, never be topped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTRTnBYaEFzx",
        "outputId": "ac03dc6d-cdc5-4d97-8316-ed6deb262bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.8616\n"
          ]
        }
      ],
      "source": [
        "train_texts = [sample[\"text\"] for sample in train_dataset]\n",
        "test_texts = [sample[\"text\"] for sample in test_dataset]\n",
        "\n",
        "# Создаем токенизатор, min_count можете взять на свое усмотрение, но лучше брать в районе 5-10;\n",
        "\n",
        "# 4. Обучаем по ним LogisticRegression, accuracy на тесте должен быть 0.75+\n",
        "\n",
        "\n",
        "# 1. Токенизатор обучаем по входным текстам\n",
        "tokenizer = Tokenizer(train_texts, min_count=5)\n",
        "# 2. Обучаем по этим же текстам с этим токенайзером TFIDF\n",
        "tfidf = TFIDF(tokenizer)\n",
        "tfidf.fit(train_texts)\n",
        "# 3. превращаем тексты в numpy матрицы\n",
        "X_train = tfidf.predict(train_texts)\n",
        "X_test = tfidf.predict(test_texts)\n",
        "\n",
        "Y_train = np.array([sample[\"label\"] for sample in train_dataset])\n",
        "Y_test = np.array([sample[\"label\"] for sample in test_dataset])\n",
        "\n",
        "# Обучаем логистическую регрессию и смотрим на качество\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, Y_train)\n",
        "prediction = clf.predict(X_test)\n",
        "\n",
        "accuracy = (prediction == Y_test).sum() / Y_test.shape[0]\n",
        "print(f\"accuracy = {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n80Zi9XLEFzx"
      },
      "source": [
        "Теперь обучим логистическую регрессию со второй TF-IDF моделью и сравним результаты. Для этого воспользуйтесь классом [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) из sklearn. Результаты должны быть похожи на вашу реализацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYX46TEkEFzx",
        "outputId": "ab31d732-af42-4b9a-f0a4-d79f67339f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.8652\n"
          ]
        }
      ],
      "source": [
        "train_texts = [sample[\"text\"] for sample in train_dataset]\n",
        "test_texts = [sample[\"text\"] for sample in test_dataset]\n",
        "tfidf = TfidfVectorizer(min_df=5)\n",
        "X_train = tfidf.fit_transform(train_texts)\n",
        "X_test = tfidf.transform(test_texts)\n",
        "\n",
        "\n",
        "Y_train = np.array([sample[\"label\"] for sample in train_dataset])\n",
        "Y_test = np.array([sample[\"label\"] for sample in test_dataset])\n",
        "\n",
        "# Обучаем логистическую регрессию и смотрим на качество\n",
        "clf = LogisticRegression(max_iter=200)\n",
        "clf.fit(X_train, Y_train)\n",
        "prediction = clf.predict(X_test)\n",
        "\n",
        "accuracy = (prediction == Y_test).sum() / Y_test.shape[0]\n",
        "print(f\"accuracy = {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsrdWu_YEFzy"
      },
      "source": [
        "## $n$-граммные языковые модели (15 баллов)\n",
        "\n",
        "### Расширяем Токенайзер (3 балла)\n",
        "\n",
        "Перед созданием языковой модели, расширим токенизационный класс. Добавим два флага в сигнатуру метода `encode`, чтобы управлять добавлением служебных токенов во время токенизации. Существующий метод `decode` уже пропускает `<PAD>` токен, добавим флаг `skip_special_tokens` для пропуска всех специальных токенов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Q46PjuLyEFzy"
      },
      "outputs": [],
      "source": [
        "class BoSTokenizerEoS(Tokenizer):\n",
        "    def encode(self, text: str, add_bos: bool = True, add_eos: bool = False) -> List[int]:\n",
        "        \"\"\"\n",
        "        Кодирование текста в набор индексов.\n",
        "\n",
        "        :param text: текст\n",
        "        :param add_bos: добавление begin-of-sentence токена в начало\n",
        "        :param add_eos: добавление end-of-sentence токена в конец\n",
        "        :return: набор индексов токенов\n",
        "        \"\"\"\n",
        "        tokens = self._tokenize(text)\n",
        "        token_ids = [self.encode_word(token) for token in tokens]\n",
        "        if add_bos:\n",
        "            token_ids = [self.word2idx[\"<BOS>\"]] + token_ids\n",
        "        if add_eos:\n",
        "            token_ids = token_ids + [self.word2idx[\"<EOS>\"]]\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, input_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
        "\n",
        "        :param input_ids: набор индексов токенов\n",
        "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
        "        :return: текст\n",
        "        \"\"\"\n",
        "        special_ids = {self.word2idx.get(\"<PAD>\"), self.word2idx.get(\"<BOS>\"),\n",
        "                       self.word2idx.get(\"<EOS>\"), self.word2idx.get(\"<UNK>\")}\n",
        "        tokens = []\n",
        "        for idx in input_ids:\n",
        "            if skip_special_tokens and idx in special_ids:\n",
        "                continue\n",
        "            tokens.append(self.idx2word[idx] if idx < len(self.idx2word) else \"<UNK>\")\n",
        "\n",
        "        return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErX6Y0gUEFzy",
        "outputId": "c78797d9-eea1-4e8d-be3c-cac7ce621c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_bos_tokenizer OK\n"
          ]
        }
      ],
      "source": [
        "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\", \"Hello there <EOS>\"]\n",
        "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
        "assert tokenizer.encode(\"hello world\", add_bos=True, add_eos=True) == [1, 4, 6, 2]\n",
        "assert tokenizer.encode(\"hello world\", add_bos=False, add_eos=False) == [4, 6]\n",
        "print(\"test_bos_tokenizer OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# double check\n",
        "BOS = tokenizer.word2idx[\"<BOS>\"]\n",
        "EOS = tokenizer.word2idx[\"<EOS>\"]\n",
        "hello = tokenizer.word2idx[\"hello\"]\n",
        "world = tokenizer.word2idx[\"world\"]\n",
        "\n",
        "assert tokenizer.encode(\"hello world\", add_bos=True, add_eos=True) == [BOS, hello, world, EOS]\n",
        "assert tokenizer.encode(\"hello world\", add_bos=False, add_eos=False) == [hello, world]\n",
        "print(\"test_bos_tokenizer OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnyfwSyWxm2z",
        "outputId": "e4f5d644-bf1b-4d99-d7df-586be581e7fa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_bos_tokenizer OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeaowalzEFzy"
      },
      "source": [
        "### Создаём NGram Модель (12 баллов)\n",
        "\n",
        "Создайте класс `NGramLanguageModel` для построения n-граммной языковой модели. В этом задании вы можете как опираться на предложенную структуру модели, так и сделать свою имплементацию.\n",
        "\n",
        "Построение модели:\n",
        "   - Создайте метод `_build_model`, который принимает список текстов `texts` и обновляет частоты n-грамм.\n",
        "   - Для каждого текста:\n",
        "     - Токенизируйте текст и добавьте токен `\"<EOS>\"` в конец.\n",
        "     - Для каждого токена:\n",
        "       - Определите префикс длиной `n-1`.\n",
        "       - Обновите частоты n-грамм и частоты префиксов.\n",
        "\n",
        "Генерация следующего токена:\n",
        "   - Создайте метод `generate_next_token`, который принимает префикс `prefix` и возвращает следующий токен.\n",
        "   - Преобразуйте префикс в кортеж.\n",
        "   - Получите распределение частот для префикса.\n",
        "   - Если распределение пустое, верните токен `\"<UNK>\"`.\n",
        "   - Верните токен с наибольшей частотой.\n",
        "   - Если включен флаг sample возьмите токен пропорционально встречаемости\n",
        "\n",
        "Автодополнение текста:\n",
        "   - Создайте метод `autocomplete`, который принимает текст `text` и максимальную длину `max_len`, и возвращает завершенный текст.\n",
        "   - Токенизируйте текст.\n",
        "   - Пока длина токенов меньше `max_len`:\n",
        "     - Определите префикс длиной `n-1`.\n",
        "     - Сгенерируйте следующий токен.\n",
        "     - Добавьте токен в список токенов.\n",
        "     - Если токен равен `\"<EOS>\"`, завершите генерацию.\n",
        "   - Декодируйте и верните текст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "s98M0YW4EFzz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class NGramLanguageModel:\n",
        "    def __init__(self, n: int, tokenizer: BoSTokenizerEoS, texts: List[str]):\n",
        "        \"\"\"\n",
        "        Создание n-граммной языковой модели.\n",
        "\n",
        "        :param n: порядок n-грамм\n",
        "        :param vocabulary: словарь\n",
        "        \"\"\"\n",
        "        assert n >= 2\n",
        "        self.n = n\n",
        "        self.tokenizer = tokenizer\n",
        "        # Словарь, ключ которого - префикс\n",
        "        # значение - Counter(), в котором ключ это продолжение n-граммы,\n",
        "        # а значение - частота встречи этого продолжения\n",
        "        self.frequencies = defaultdict(lambda: Counter())  # частота n-грамм\n",
        "        self._build_model(texts)\n",
        "\n",
        "    def _build_model(self, texts: List[str]):\n",
        "        \"\"\"\n",
        "        Построение модели на основе списка текстов.\n",
        "\n",
        "        :param texts: список текстов\n",
        "        \"\"\"\n",
        "        for text in texts:\n",
        "          token_ids = self.tokenizer.encode(text, add_bos=True, add_eos=True)\n",
        "          for i in range(len(token_ids) - self.n + 1):\n",
        "            prefix = tuple(token_ids[i:i+self.n-1])\n",
        "            next_token = token_ids[i + self.n - 1]\n",
        "            self.frequencies[prefix][next_token] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_next_token(self, prefix: List[int], sample: bool = False) -> int:\n",
        "        \"\"\"\n",
        "        Генерация следующего токена по префиксу.\n",
        "\n",
        "        :param prefix: префикс\n",
        "        :param sample: используем ли мы сэмплинг в генерации\n",
        "        :return: следующий токен\n",
        "        \"\"\"\n",
        "        prefix = tuple(prefix[-(self.n - 1):])\n",
        "        counter = self.frequencies[prefix]\n",
        "        if not counter:\n",
        "            return self.tokenizer.word2idx[\"<EOS>\"]\n",
        "        if sample:\n",
        "            tokens, freqs = zip(*counter.items())\n",
        "            total = sum(freqs)\n",
        "            probs = [f / total for f in freqs]\n",
        "            return random.choices(tokens, probs)[0]\n",
        "        else:\n",
        "          return counter.most_common(1)[0][0]\n",
        "\n",
        "    def autocomplete(self, text: str, max_len: int = 32,\n",
        "                     skip_special_tokens: bool = True, sample: bool = False) -> str:\n",
        "        \"\"\"\n",
        "        Автоматическое дополнение текста.\n",
        "\n",
        "        :param text: текст\n",
        "        :param max_len: максимальная длина текста\n",
        "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
        "        :return: завершенный текст\n",
        "        \"\"\"\n",
        "        token_ids = self.tokenizer.encode(text, add_bos=True, add_eos=False)\n",
        "        for _ in range(max_len):\n",
        "            prefix = token_ids[-(self.n-1):] if len(token_ids) >= self.n-1 else token_ids\n",
        "            next_token = self.generate_next_token(prefix, sample=sample)\n",
        "            token_ids.append(next_token)\n",
        "            if next_token == self.tokenizer.word2idx[\"<EOS>\"]:\n",
        "                break\n",
        "        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUMKN0CVEFzz",
        "outputId": "21b23054-83d6-48bf-8d09-67c16b76c7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Продолжение фразы `the movie was`\n",
            "the movie was bad . < br / > the musical score that enhances the quality of the four . jessica is always put moonstruck on when there is no other version of the movie\n",
            "test_ngram_model OK\n"
          ]
        }
      ],
      "source": [
        "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\"]\n",
        "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
        "ngram_lm = NGramLanguageModel(2, tokenizer, corpus)\n",
        "\n",
        "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10) == \"hello , python !\"\n",
        "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10, skip_special_tokens=False) == \"<BOS> hello , python ! <EOS>\"\n",
        "\n",
        "imdb = load_dataset(\"imdb\")\n",
        "train_texts = [imdb[\"train\"][i][\"text\"] for i in range(13_000, 15_000)]\n",
        "tokenizer = BoSTokenizerEoS(texts=train_texts, min_count=3)\n",
        "ngram_lm = NGramLanguageModel(3, tokenizer, train_texts)\n",
        "print(\"Продолжение фразы `the movie was`\")\n",
        "random.seed(1)\n",
        "print(ngram_lm.autocomplete(\"the movie was\", sample=True))\n",
        "print(\"test_ngram_model OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVrA1vyoEFzz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}